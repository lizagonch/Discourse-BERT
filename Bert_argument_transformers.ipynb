{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Bert_argument_transformers.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "YrpkDbgbmx2t"
      },
      "source": [
        "import zipfile\n",
        "import os\n",
        "import time\n",
        "\n",
        "from google.colab import drive"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VKYiwM28wfnK",
        "outputId": "457c8a4b-434d-4c94-bed0-a1ae0152380f"
      },
      "source": [
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QDwWtTpTwk8I",
        "outputId": "31284c7a-6e52-4f3d-d933-10d0ecd764e7"
      },
      "source": [
        "zip_file = '/content/drive/MyDrive/cbert_aug-crayon.zip'\n",
        "\n",
        "z = zipfile.ZipFile(zip_file, 'r')\n",
        "z.extractall()\n",
        "\n",
        "print(os.listdir())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['.config', '__pycache__', 'aug_data', 'cbert_utils.py', 'cbert_pretrained', 'drive', 'datasets', 'train_text_classifier.py', '.ipynb_checkpoints', 'utils.py', 'global.config', 'nets.py', 'args_of_text_classifier.py', 'cbert_augdata.py', 'text_classification', 'cbert_finetune.py', 'evaluator.py', 'triggers.py', 'README.md', 'sample_data']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kd9MIsrIyBKG"
      },
      "source": [
        "import torch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "omL6foCWyENU",
        "outputId": "82ad45d5-8fc1-440f-bfb2-afb5f2ce491a"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.5.1)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (3.10.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.45)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FRjOWz3jyHg1",
        "outputId": "40ea910d-159d-4b16-f9a2-6de3e0e1f9c5"
      },
      "source": [
        "! pip install ipdb"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: ipdb in /usr/local/lib/python3.7/dist-packages (0.13.7)\n",
            "Requirement already satisfied: ipython>=7.17.0; python_version > \"3.6\" in /usr/local/lib/python3.7/dist-packages (from ipdb) (7.22.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from ipdb) (54.2.0)\n",
            "Requirement already satisfied: toml>=0.10.2; python_version > \"3.6\" in /usr/local/lib/python3.7/dist-packages (from ipdb) (0.10.2)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.7/dist-packages (from ipython>=7.17.0; python_version > \"3.6\"->ipdb) (5.0.5)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython>=7.17.0; python_version > \"3.6\"->ipdb) (2.6.1)\n",
            "Requirement already satisfied: pexpect>4.3; sys_platform != \"win32\" in /usr/local/lib/python3.7/dist-packages (from ipython>=7.17.0; python_version > \"3.6\"->ipdb) (4.8.0)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from ipython>=7.17.0; python_version > \"3.6\"->ipdb) (3.0.18)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.7/dist-packages (from ipython>=7.17.0; python_version > \"3.6\"->ipdb) (0.2.0)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython>=7.17.0; python_version > \"3.6\"->ipdb) (0.7.5)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython>=7.17.0; python_version > \"3.6\"->ipdb) (4.4.2)\n",
            "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.7/dist-packages (from ipython>=7.17.0; python_version > \"3.6\"->ipdb) (0.18.0)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.7/dist-packages (from traitlets>=4.2->ipython>=7.17.0; python_version > \"3.6\"->ipdb) (0.2.0)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect>4.3; sys_platform != \"win32\"->ipython>=7.17.0; python_version > \"3.6\"->ipdb) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=7.17.0; python_version > \"3.6\"->ipdb) (0.2.5)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from jedi>=0.16->ipython>=7.17.0; python_version > \"3.6\"->ipdb) (0.8.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FyoIJlnDyLBt"
      },
      "source": [
        "os.chdir('./')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Twf2tl8DyN3f",
        "outputId": "8732f317-2fa0-4fe2-e74e-b917f589315b"
      },
      "source": [
        "!ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "args_of_text_classifier.py  datasets\t   README.md\n",
            "aug_data\t\t    drive\t   sample_data\n",
            "cbert_augdata.py\t    evaluator.py   text_classification\n",
            "cbert_finetune.py\t    global.config  train_text_classifier.py\n",
            "cbert_pretrained\t    nets.py\t   triggers.py\n",
            "cbert_utils.py\t\t    __pycache__    utils.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iF02GRwed5h0"
      },
      "source": [
        "import pandas as pd\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bLCPyEAVde1T",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380
        },
        "outputId": "f3efc16c-dd1d-4b2d-861d-38a9ed0df735"
      },
      "source": [
        "sample_data = pd.read_csv('data (3).tsv', sep='\\t')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-44-d36bac41acc0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msample_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data (3).tsv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'\\t'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    686\u001b[0m     )\n\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 688\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    689\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 454\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    946\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 948\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    950\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1178\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1180\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1181\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   2008\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2009\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2010\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2011\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2012\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data (3).tsv'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 662
        },
        "id": "LhdD2STjd8j7",
        "outputId": "0dbf48fa-4cf2-49a5-8a04-f3dda0a0f264"
      },
      "source": [
        "sample_data.head(20)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>words</th>\n",
              "      <th>rst</th>\n",
              "      <th>class</th>\n",
              "      <th>segment</th>\n",
              "      <th>label</th>\n",
              "      <th>code_class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>school uniforms is good. Or drag a student ove...</td>\n",
              "      <td>none</td>\n",
              "      <td>Argument_against</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>school uniforms is good.</td>\n",
              "      <td>attribution</td>\n",
              "      <td>Argument_against</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>school uniforms is good.</td>\n",
              "      <td>none</td>\n",
              "      <td>Argument_against</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Only</td>\n",
              "      <td>attribution</td>\n",
              "      <td>Argument_against</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>for prostitutes is the school uniform an oblig...</td>\n",
              "      <td>none</td>\n",
              "      <td>Argument_against</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>why ) .</td>\n",
              "      <td>elaboration</td>\n",
              "      <td>Argument_against</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>school uniforms is good. The story currently r...</td>\n",
              "      <td>none</td>\n",
              "      <td>NoArgument</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>resulting in a large number of regular inquiri...</td>\n",
              "      <td>elaboration</td>\n",
              "      <td>NoArgument</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>one on Google , Yahoo and Bing for the search ...</td>\n",
              "      <td>elaboration</td>\n",
              "      <td>NoArgument</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>school uniforms is good. Some might say , Wait...</td>\n",
              "      <td>none</td>\n",
              "      <td>NoArgument</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>school uniforms is good. In a book published i...</td>\n",
              "      <td>none</td>\n",
              "      <td>NoArgument</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>to set the record straight on what uniforms ca...</td>\n",
              "      <td>purpose</td>\n",
              "      <td>NoArgument</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>school uniforms is good. Why would we put that...</td>\n",
              "      <td>none</td>\n",
              "      <td>NoArgument</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>school uniforms is good. Editorial Note : Any ...</td>\n",
              "      <td>none</td>\n",
              "      <td>Argument_for</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>endorsed by any card issuer .</td>\n",
              "      <td>elaboration</td>\n",
              "      <td>Argument_for</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>Any opinions , analyses , reviews or recommend...</td>\n",
              "      <td>elaboration</td>\n",
              "      <td>Argument_for</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>school uniforms is good. And God save the brav...</td>\n",
              "      <td>none</td>\n",
              "      <td>NoArgument</td>\n",
              "      <td>7</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>who daily dons a modest skirt below her knees .</td>\n",
              "      <td>elaboration</td>\n",
              "      <td>NoArgument</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>who dares to buck the trend and wear a suit ja...</td>\n",
              "      <td>elaboration</td>\n",
              "      <td>NoArgument</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>to nigh heroic , young man who dares to buck t...</td>\n",
              "      <td>elaboration</td>\n",
              "      <td>NoArgument</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                words  ... code_class\n",
              "0   school uniforms is good. Or drag a student ove...  ...          0\n",
              "1                            school uniforms is good.  ...          0\n",
              "2                            school uniforms is good.  ...          0\n",
              "3                                                Only  ...          0\n",
              "4   for prostitutes is the school uniform an oblig...  ...          0\n",
              "5                                             why ) .  ...          0\n",
              "6   school uniforms is good. The story currently r...  ...          2\n",
              "7   resulting in a large number of regular inquiri...  ...          2\n",
              "8   one on Google , Yahoo and Bing for the search ...  ...          2\n",
              "9   school uniforms is good. Some might say , Wait...  ...          2\n",
              "10  school uniforms is good. In a book published i...  ...          2\n",
              "11  to set the record straight on what uniforms ca...  ...          2\n",
              "12  school uniforms is good. Why would we put that...  ...          2\n",
              "13  school uniforms is good. Editorial Note : Any ...  ...          1\n",
              "14                      endorsed by any card issuer .  ...          1\n",
              "15  Any opinions , analyses , reviews or recommend...  ...          1\n",
              "16  school uniforms is good. And God save the brav...  ...          2\n",
              "17    who daily dons a modest skirt below her knees .  ...          2\n",
              "18  who dares to buck the trend and wear a suit ja...  ...          2\n",
              "19  to nigh heroic , young man who dares to buck t...  ...          2\n",
              "\n",
              "[20 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "07OUBYXV4JTo"
      },
      "source": [
        "import os\n",
        "import csv\n",
        "import logging\n",
        "import random\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler\n",
        "\n",
        "\"\"\"initialize logger\"\"\"\n",
        "logging.basicConfig(format='%(asctime)s - %(levelname)s - %(name)s - %(message)s',\n",
        "                    datefmt='%m/%d/%Y %H:%M:%S',\n",
        "                    level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "class InputExample(object):\n",
        "    \"\"\"A single training/test example for simple sequence classification.\"\"\"\n",
        "    def __init__(self, guid, text_a, segment, cl, label=None):\n",
        "        \"\"\"Constructs a InputExample/\n",
        "\n",
        "        Args:\n",
        "            guid: Unique id for the example.\n",
        "            text: string. The untokenized text of the first sequence. For single\n",
        "            sequence tasks, only this sequence must be specified.\n",
        "        \"\"\"\n",
        "        self.guid = guid\n",
        "        self.text_a = text_a\n",
        "        self.label = label\n",
        "        self.segment = segment\n",
        "        self.cl = cl\n",
        "\n",
        "class InputFeature(object):\n",
        "    \"\"\"A single set of features of data.\"\"\"\n",
        "\n",
        "    def __init__(self, init_ids, input_ids, input_mask, segment_ids, masked_lm_labels, segment_span, class_span):\n",
        "        self.init_ids = init_ids\n",
        "        self.input_ids = input_ids\n",
        "        self.input_mask = input_mask\n",
        "        self.segment_ids = segment_ids\n",
        "        self.masked_lm_labels = masked_lm_labels\n",
        "        self.segment_span = segment_span\n",
        "        self.class_span = class_span\n",
        "\n",
        "class DataProcessor(object):\n",
        "    \"\"\"Base class for data converters for sequence classification data sets.\"\"\"\n",
        "\n",
        "    def get_train_examples(self, data_dir):\n",
        "        \"\"\"Gets a collection of 'InputExample's for the train set.\"\"\"\n",
        "        raise NotImplementedError()\n",
        "\n",
        "    def get_dev_examples(self, data_dir):\n",
        "        \"\"\"Gets a collection of 'InputExample's for the dev set.\"\"\"\n",
        "        raise NotImplementedError()\n",
        "\n",
        "    def get_labels(self):\n",
        "        \"\"\"Gets the list of labels for this data set.\"\"\"\n",
        "        raise NotImplementedError()\n",
        "\n",
        "    @classmethod\n",
        "    def _read_tsv(cls, input_file, quotechar=None):\n",
        "        \"\"\"Reads a tab separated value file.\"\"\"\n",
        "        with open(input_file, \"r\") as f:\n",
        "            reader = csv.reader(f, delimiter=\"\\t\", quotechar=quotechar)\n",
        "            lines = []\n",
        "            for line in reader:\n",
        "                lines.append(line)\n",
        "            return lines\n",
        "\n",
        "class AugProcessor(DataProcessor):\n",
        "    \"\"\"Processor for dataset to be augmented.\"\"\"\n",
        "\n",
        "    def get_train_examples(self, data_dir):\n",
        "        \"\"\"See base calss.\"\"\"\n",
        "        return self._create_examples(\n",
        "            self._read_tsv(os.path.join(data_dir, \"train.tsv\")), \"train\")\n",
        "\n",
        "    def get_dev_examples(self, data_dir):\n",
        "        \"\"\"See base class.\"\"\"\n",
        "        return self._create_examples(\n",
        "            self._read_tsv(os.path.join(data_dir, \"dev.tsv\")), \"dev\")\n",
        "    \n",
        "    def get_labels(self, name):\n",
        "        \"\"\"add your dataset here\"\"\"\n",
        "        if name in ['stsa.binary', 'mpqa', 'rt-polarity', 'subj']:\n",
        "            return [\"0\", \"1\"]\n",
        "        elif name in ['stsa.fine']:\n",
        "            return [\"0\", \"1\", \"2\", \"3\", \"4\"]\n",
        "        elif name in ['TREC']:\n",
        "            return [\"0\", \"1\", \"2\", \"3\", \"4\", \"5\"]\n",
        "        elif name in ['amazon']:\n",
        "            print(\"Amazon\")\n",
        "            return [\"0\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\", \"10\", \"11\", \"12\"]\n",
        "  \n",
        "    def _create_examples(self, lines, set_type):\n",
        "        \"\"\"Create examples for the training and dev sets.\"\"\"\n",
        "        examples = []\n",
        "        for (i, line) in enumerate(lines):\n",
        "            if i == 0:\n",
        "                continue\n",
        "            guid = \"%s-%s\" % (set_type, i)\n",
        "            text_a = line[0]\n",
        "            segment = line[-3]\n",
        "            #print(segment)\n",
        "            label = line[-2]\n",
        "            #print(label)\n",
        "            cl = line[-1]\n",
        "            #print(cl)\n",
        "            examples.append(\n",
        "                InputExample(guid=guid, text_a=text_a, segment = segment, cl = cl, label=label))\n",
        "        return examples\n",
        "\n",
        "def convert_examples_to_features(examples, label_list, max_seq_length, tokenizer):\n",
        "    \"\"\"Loads a data file into a list of 'InputBatch's.\"\"\"\n",
        "    \n",
        "    label_map = {}\n",
        "    for (i, label) in enumerate(label_list):\n",
        "        label_map[label] = i\n",
        "    \n",
        "    features = []\n",
        "    for (ex_index, example) in enumerate(examples):\n",
        "        # The convention in BERT is:\n",
        "        # tokens:   [CLS] is this jack ##son ##ville ? [SEP]\n",
        "        # type_ids: 0     0  0    0    0     0       0 0    \n",
        "        tokens_a = tokenizer._tokenize(example.text_a)\n",
        "        tokens_label = label_map[example.label]\n",
        "        tokens, init_ids, input_ids, input_mask, segment_ids, masked_lm_labels, segment_span, class_span = extract_features(tokens_a, tokens_label, example.segment, example.cl, max_seq_length, tokenizer)\n",
        "\n",
        "        #print(\"where is a mistake, I wonder...\")\n",
        "        \n",
        "        \"\"\"convert label to label_id\"\"\"\n",
        "        label_id = label_map[example.label]\n",
        "\n",
        "        \"\"\"consturct features\"\"\"\n",
        "        features.append(\n",
        "            InputFeature(\n",
        "                init_ids=init_ids,        \n",
        "                input_ids=input_ids,\n",
        "                input_mask=input_mask,\n",
        "                segment_ids=segment_ids,\n",
        "                masked_lm_labels=masked_lm_labels,\n",
        "                segment_span = segment_span,\n",
        "                class_span = class_span))\n",
        "\n",
        "        \"\"\"print examples\"\"\"\n",
        "        if ex_index < 5:\n",
        "            logger.info(\"[cbert] *** Example ***\")\n",
        "            logger.info(\"[cbert] guid: %s\" % (example.guid))\n",
        "            logger.info(\"[cbert] tokens: %s\" % \" \".join(\n",
        "                [str(x) for x in tokens]))\n",
        "            logger.info(\"[cbert] init_ids: %s\" % \" \".join([str(x) for x in init_ids]))\n",
        "            logger.info(\"[cbert] input_ids: %s\" % \" \".join([str(x) for x in input_ids]))\n",
        "            logger.info(\"[cbert] input_mask: %s\" % \" \".join([str(x) for x in input_mask]))\n",
        "            logger.info(\"[cbert] segment_ids: %s\" % \" \".join([str(x) for x in segment_ids]))\n",
        "            logger.info(\"[cbert] masked_lm_labels: %s\" % \" \".join([str(x) for x in masked_lm_labels]))\n",
        "    return features\n",
        "\n",
        "def construct_train_dataloader(train_examples, label_list, max_seq_length, train_batch_size, num_train_epochs, tokenizer, device):\n",
        "    \"\"\"construct dataloader for training data\"\"\"\n",
        "    \n",
        "    num_train_steps = None\n",
        "    global_step = 0\n",
        "    train_features = convert_examples_to_features(\n",
        "        train_examples, label_list, max_seq_length, tokenizer)\n",
        "    num_train_steps = int(len(train_features) / train_batch_size * num_train_epochs)\n",
        "\n",
        "    #print(train_features)\n",
        "    #for f in train_features:\n",
        "    #  print(f.init_ids)\n",
        "    \n",
        "    all_init_ids = torch.tensor([f.init_ids for f in train_features], dtype=torch.long, device=device)\n",
        "    #print([f.init_ids for f in train_features])\n",
        "    all_input_ids = torch.tensor([f.input_ids for f in train_features], dtype=torch.long, device=device)\n",
        "    all_input_mask = torch.tensor([f.input_mask for f in train_features], dtype=torch.long, device=device)\n",
        "    all_segment_ids = torch.tensor([f.segment_ids for f in train_features], dtype=torch.long, device=device)\n",
        "    all_masked_lm_labels = torch.tensor([f.masked_lm_labels for f in train_features], dtype=torch.long, device=device)\n",
        "    segment_span = torch.tensor([int(f.segment_span) for f in train_features], dtype=torch.long, device=device)\n",
        "    class_span = torch.tensor([int(f.class_span) for f in train_features], dtype=torch.long, device=device)\n",
        "    \n",
        "    tensor_dataset = TensorDataset(all_init_ids, all_input_ids, all_input_mask, \n",
        "        all_segment_ids, all_masked_lm_labels, segment_span, class_span)\n",
        "    train_sampler = RandomSampler(tensor_dataset)\n",
        "    train_dataloader = DataLoader(tensor_dataset, sampler=train_sampler, batch_size=train_batch_size)\n",
        "    return train_features, num_train_steps, train_dataloader\n",
        "\n",
        "def rev_wordpiece(str):\n",
        "    \"\"\"wordpiece function used in cbert\"\"\"\n",
        "    \n",
        "    #print(str)\n",
        "    if len(str) > 1:\n",
        "        for i in range(len(str)-1, 0, -1):\n",
        "            if str[i] == '[PAD]':\n",
        "                str.remove(str[i])\n",
        "            elif len(str[i]) > 1 and str[i][0]=='#' and str[i][1]=='#':\n",
        "                str[i-1] += str[i][2:]\n",
        "                str.remove(str[i])\n",
        "    return \" \".join(str[1:-1])\n",
        "\n",
        "\n",
        "def extract_features(tokens_a, tokens_label, segment, cl, max_seq_length, tokenizer):\n",
        "    \"\"\"extract features from tokens\"\"\"\n",
        "\n",
        "    if len(tokens_a) > max_seq_length - 2:\n",
        "        tokens_a = tokens_a[0: (max_seq_length - 2)]\n",
        "    \n",
        "    tokens = []\n",
        "    segment_ids = []\n",
        "    tokens.append('[CLS]')\n",
        "    segment_ids.append(tokens_label)\n",
        "    for token in tokens_a:\n",
        "        tokens.append(token)\n",
        "        segment_ids.append(tokens_label)\n",
        "    tokens.append('[SEP]')\n",
        "    segment_ids.append(tokens_label)\n",
        "\n",
        "    ## construct init_ids for each example\n",
        "    init_ids = convert_tokens_to_ids(tokens, tokenizer)\n",
        "\n",
        "    ## construct input_ids for each example, we replace the word_id using \n",
        "    ## the ids of masked words (mask words based on original sentence)\n",
        "    masked_lm_probs = 0.15\n",
        "    max_predictions_per_seq = 20\n",
        "    rng = random.Random(12345)\n",
        "    original_masked_lm_labels = [-100] * max_seq_length\n",
        "    (output_tokens, masked_lm_positions, \n",
        "    masked_lm_labels) = create_masked_lm_predictions(\n",
        "            tokens, masked_lm_probs, original_masked_lm_labels, max_predictions_per_seq, rng, tokenizer)\n",
        "    input_ids = convert_tokens_to_ids(output_tokens, tokenizer)\n",
        "\n",
        "    # The mask has 1 for real tokens and 0 for padding tokens. Only real\n",
        "    # tokens are attended to.\n",
        "    input_mask = [1] * len(input_ids)\n",
        "    \n",
        "    # Zero-pad up to the sequence length.\n",
        "    while len(input_ids) < max_seq_length:\n",
        "        init_ids.append(0)\n",
        "        input_ids.append(0)\n",
        "        input_mask.append(0)\n",
        "        segment_ids.append(0)\n",
        "    \n",
        "    assert len(init_ids) == max_seq_length\n",
        "    assert len(input_ids) == max_seq_length\n",
        "    assert len(input_mask) == max_seq_length\n",
        "    assert len(segment_ids) == max_seq_length\n",
        "\n",
        "    return tokens, init_ids, input_ids, input_mask, segment_ids, masked_lm_labels, segment, cl\n",
        "\n",
        "def convert_tokens_to_ids(tokens, tokenizer):\n",
        "    \"\"\"Converts tokens into ids using the vocab.\"\"\"\n",
        "    ids = []\n",
        "    for token in tokens:\n",
        "        token_id = tokenizer._convert_token_to_id(token)\n",
        "        ids.append(token_id)\n",
        "    return ids\n",
        "\n",
        "def create_masked_lm_predictions(tokens, masked_lm_probs, masked_lm_labels, \n",
        "                                 max_predictions_per_seq, rng, tokenizer):\n",
        "    \"\"\"Creates the predictions for the masked LM objective.\"\"\"\n",
        "\n",
        "    #vocab_words = list(tokenizer.vocab.keys())\n",
        "    \n",
        "    cand_indexes = []\n",
        "    for (i, token) in enumerate(tokens):\n",
        "        if token == \"[CLS]\" or token == \"[SEP]\":\n",
        "            continue\n",
        "        cand_indexes.append(i)\n",
        "    \n",
        "    rng.shuffle(cand_indexes)\n",
        "    len_cand = len(cand_indexes)\n",
        "    output_tokens = list(tokens)\n",
        "    num_to_predict = min(max_predictions_per_seq, \n",
        "                         max(1, int(round(len(tokens) * masked_lm_probs))))\n",
        "    \n",
        "    masked_lm_positions = []\n",
        "    covered_indexes = set()\n",
        "    for index in cand_indexes:\n",
        "        if len(masked_lm_positions) >= num_to_predict:\n",
        "            break\n",
        "        if index in covered_indexes:\n",
        "            continue\n",
        "        covered_indexes.add(index)\n",
        "\n",
        "        masked_token = None\n",
        "        ## 80% of the time, replace with [MASK]\n",
        "        if rng.random() < 0.8:\n",
        "            masked_token = \"[MASK]\"\n",
        "        else:\n",
        "            ## 10% of the time, keep original\n",
        "            if rng.random() < 0.5:\n",
        "                masked_token = tokens[index]\n",
        "            ## 10% of the time, replace with random word\n",
        "            else:\n",
        "                masked_token = tokens[cand_indexes[rng.randint(0, len_cand - 1)]]\n",
        "                \n",
        "        masked_lm_labels[index] = convert_tokens_to_ids([tokens[index]], tokenizer)[0]\n",
        "        output_tokens[index] = masked_token\n",
        "        masked_lm_positions.append(index)\n",
        "    return output_tokens, masked_lm_positions, masked_lm_labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TpE5muKo47BN"
      },
      "source": [
        "class Arguments(object):\n",
        "    \"\"\"A single training/test example for simple sequence classification.\"\"\"\n",
        "\n",
        "    def __init__(self, data_dir=\"datasets\", output_dir=\"aug_data\", save_model_dir = \"cbert_pretrained\", bert_model=\"bert-base-uncased\", task_name=\"subj\", max_seq_length=64, do_lower_case=False, train_batch_size=32, learning_rate=5e-5, num_train_epochs=10.0, warmup_proportion=0.1, seed=42, save_every_epoch = True):\n",
        "        \"\"\"Constructs a InputExample.\n",
        "\n",
        "        Args:\n",
        "            guid: Unique id for the example.\n",
        "            text_a: string. The untokenized text of the first sequence. For single\n",
        "            sequence tasks, only this sequence must be specified.\n",
        "        \"\"\"\n",
        "        self.data_dir = data_dir\n",
        "        self.output_dir = output_dir\n",
        "        self.save_model_dir = save_model_dir\n",
        "        self.bert_model = bert_model\n",
        "        self.task_name = task_name\n",
        "        self.max_seq_length = max_seq_length\n",
        "        self.do_lower_case = do_lower_case\n",
        "        self.train_batch_size = train_batch_size\n",
        "        self.learning_rate = learning_rate\n",
        "        self.num_train_epochs = num_train_epochs\n",
        "        self.warmup_proportion = warmup_proportion\n",
        "        self.seed = seed\n",
        "        self.save_every_epoch = save_every_epoch\n",
        "        self.sample_num = 1\n",
        "        self.sample_ratio = 7\n",
        "        self.gpu = 0\n",
        "        self.temp = 1.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 434
        },
        "id": "YB_LGFTtyO_G",
        "outputId": "e27c0d7b-d1c8-4716-9f27-10e7c084584b"
      },
      "source": [
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import os\n",
        "import shutil\n",
        "import logging\n",
        "import argparse\n",
        "import random\n",
        "import json\n",
        "from tqdm import tqdm, trange\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "from transformers import BertTokenizer, BertModel, BertForMaskedLM, AdamW#, WarmupLinearSchedule\n",
        "#import train_text_classifier_new\n",
        "\n",
        "#import cbert_utils\n",
        "\n",
        "\"\"\"initialize logger\"\"\"\n",
        "logging.basicConfig(format='%(asctime)s - %(levelname)s - %(name)s - %(message)s',\n",
        "                    datefmt='%m/%d/%Y %H:%M:%S',\n",
        "                    level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "\"\"\"cuda or cpu\"\"\"\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "\n",
        "def main():\n",
        "    #parser = argparse.ArgumentParser()\n",
        "\n",
        "    ## Required parameters\n",
        "    #parser.add_argument(\"--data_dir\", default=\"datasets\", type=str,\n",
        "    #                    help=\"The input data dir. Should contain the .tsv files (or other data files) for the task.\")\n",
        "    #parser.add_argument(\"--output_dir\", default=\"aug_data\", type=str,\n",
        "    #                    help=\"The output dir for augmented dataset.\")\n",
        "    #parser.add_argument(\"--save_model_dir\", default=\"cbert_model\", type=str,\n",
        "    #                    help=\"The cache dir for saved model.\")\n",
        "    #parser.add_argument(\"--bert_model\", default=\"bert-base-uncased\", type=str,\n",
        "    #                    help=\"The path of pretrained bert model.\")\n",
        "    #parser.add_argument(\"--task_name\", default=\"subj\", type=str,\n",
        "    #                    help=\"The name of the task to train.\")\n",
        "    #parser.add_argument(\"--max_seq_length\", default=64, type=int,\n",
        "    #                    help=\"The maximum total input sequence length after WordPiece tokenization. \\n\"\n",
        "    #                         \"Sequence longer than this will be truncated, and sequences shorter \\n\"\n",
        "    #                         \"than this wille be padded.\")\n",
        "    #parser.add_argument(\"--do_lower_case\", default=False, action='store_true',\n",
        "    #                    help=\"Set this flag if you are using an uncased model.\")\n",
        "    #parser.add_argument(\"--train_batch_size\", default=32, type=int,\n",
        "    #                    help=\"Total batch size for training.\")\n",
        "    #parser.add_argument(\"--learning_rate\", default=5e-5, type=float,\n",
        "    #                    help=\"The initial learning rate for Adam.\")\n",
        "    #parser.add_argument(\"--num_train_epochs\", default=10.0, type=float,\n",
        "    #                    help=\"Total number of training epochs to perform.\")\n",
        "    #parser.add_argument(\"--warmup_proportion\", default=0.1, type=float,\n",
        "    #                    help=\"Proportion of training to perform linear learning rate warmup for.\"\n",
        "    #                         \"E.g., 0.1 = 10%% of training.\")\n",
        "    #parser.add_argument(\"--seed\", type=int, default=42,\n",
        "    #                    help=\"random seed for initialization\")\n",
        "    #parser.add_argument(\"--save_every_epoch\", default=True, action='store_true')\n",
        "\n",
        "    #args = parser.parse_args()\n",
        "\n",
        "    with open(\"global.config\", 'r') as f:\n",
        "        configs_dict = json.load(f)\n",
        "    \n",
        "    print(configs_dict)\n",
        "    args = Arguments()\n",
        "    args.task_name = configs_dict.get(\"dataset\")\n",
        "    args.save_model_dir = \"cbert_pretrained\"\n",
        "    print(args)\n",
        "    \n",
        "    \"\"\"prepare processors\"\"\"\n",
        "    augProcessor = AugProcessor()\n",
        "    processors = {\n",
        "        ## you can add your processor here\n",
        "        \"TREC\": augProcessor,\n",
        "        \"stsa.fine\": augProcessor,\n",
        "        \"stsa.binary\": augProcessor,\n",
        "        \"mpqa\": augProcessor,\n",
        "        \"rt-polarity\": augProcessor,\n",
        "        \"subj\": augProcessor,\n",
        "        \"amazon\": augProcessor,\n",
        "    }\n",
        "\n",
        "    task_name = args.task_name\n",
        "    if task_name not in processors:\n",
        "        raise ValueError(\"Task not found: %s\" % (task_name))\n",
        "    processor = processors[task_name]\n",
        "    print(\"task_name\", task_name)\n",
        "    label_list = processor.get_labels(task_name)\n",
        "    print(\"label_list\", label_list)\n",
        "\n",
        "    \"\"\"prepare model\"\"\"\n",
        "    random.seed(args.seed)\n",
        "    np.random.seed(args.seed)\n",
        "    torch.manual_seed(args.seed)\n",
        "\n",
        "    ## leveraging lastest bert module in Transformers to load pre-trained model tokenizer\n",
        "    tokenizer = BertTokenizer.from_pretrained(args.bert_model)\n",
        "\n",
        "    print(\"bert\", args.bert_model)\n",
        "    \n",
        "    ## leveraging lastest bert module in Transformers to load pre-trained model (weights)\n",
        "    model = BertForMaskedLM.from_pretrained(args.bert_model, output_hidden_states = True)\n",
        "\n",
        "    if task_name == 'stsa.fine':\n",
        "        model.bert.embeddings.token_type_embeddings = torch.nn.Embedding(5, 768)\n",
        "        model.bert.embeddings.token_type_embeddings.weight.data.normal_(mean=0.0, std=0.02)\n",
        "    elif task_name == 'TREC':\n",
        "        model.bert.embeddings.token_type_embeddings = torch.nn.Embedding(6, 768)\n",
        "        model.bert.embeddings.token_type_embeddings.weight.data.normal_(mean=0.0, std=0.02)\n",
        "    elif task_name == 'amazon':\n",
        "        model.bert.embeddings.token_type_embeddings = torch.nn.Embedding(13, 768)\n",
        "        model.bert.embeddings.token_type_embeddings.weight.data.normal_(mean=0.0, std=0.02)\n",
        "\n",
        "    args.data_dir = os.path.join(args.data_dir, task_name)\n",
        "    args.output_dir = os.path.join(args.output_dir, task_name)\n",
        "    os.makedirs(args.output_dir, exist_ok=True)\n",
        "\n",
        "    print(args.data_dir)\n",
        "    train_examples = processor.get_train_examples(args.data_dir)\n",
        "    train_features, num_train_steps, train_dataloader = construct_train_dataloader(train_examples, label_list, args.max_seq_length, args.train_batch_size, args.num_train_epochs, tokenizer, device)\n",
        "    \n",
        "    model.cuda()\n",
        "    #global_step = 0\n",
        "    #train_features = convert_examples_to_features(\n",
        "    #    train_examples, label_list, args.max_seq_length, tokenizer)\n",
        "    #num_train_steps = int(len(train_features) / args.train_batch_size * args.num_train_epochs)\n",
        "    \n",
        "    #logger.info(\"***** Running training *****\")\n",
        "    #logger.info(\"  Num examples = %d\", len(train_features))\n",
        "    #logger.info(\"  Batch size = %d\", args.train_batch_size)\n",
        "    #logger.info(\"  Num steps = %d\", num_train_steps)\n",
        "    #all_init_ids = torch.tensor([f.init_ids for f in train_features], dtype=torch.long)\n",
        "    #all_input_ids = torch.tensor([f.input_ids for f in train_features], dtype=torch.long)\n",
        "    #all_input_mask = torch.tensor([f.input_mask for f in train_features], dtype=torch.long)\n",
        "    #all_segment_ids = torch.tensor([f.segment_ids for f in train_features], dtype=torch.long)\n",
        "    #all_masked_lm_labels = torch.tensor([f.masked_lm_labels for f in train_features], dtype=torch.long)\n",
        "    #train_data = TensorDataset(all_init_ids, all_input_ids, all_input_mask, all_segment_ids, all_masked_lm_labels)\n",
        "    #train_sampler = RandomSampler(train_data)\n",
        "    #train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=args.train_batch_size)\n",
        "\n",
        "    ## if you have a GPU, put everything on cuda\n",
        "    #model.cuda()\n",
        "\n",
        "    print(\"batch\", args.train_batch_size)\n",
        "\n",
        "    logger.info(\"***** Running training *****\")\n",
        "    logger.info(\"  Num examples = %d\", len(train_features))\n",
        "    logger.info(\"  Batch size = %d\", args.train_batch_size)\n",
        "    logger.info(\"  Num steps = %d\", num_train_steps)\n",
        "\n",
        "    ## in Transformers, optimizer and schedules are splitted and instantiated like this:\n",
        "    param_optimizer = list(model.named_parameters())\n",
        "    no_decay = ['bias', 'gamma', 'beta']\n",
        "    optimizer_grounded_parameters = [\n",
        "        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay_rate': 0.01},\n",
        "        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay_rate': 0.0}\n",
        "    ]\n",
        "    optimizer = AdamW(optimizer_grounded_parameters, lr=args.learning_rate, correct_bias=False)\n",
        "    model.train()\n",
        "\n",
        "    os.makedirs(args.save_model_dir, exist_ok=True)\n",
        "    save_model_dir = os.path.join(args.save_model_dir, task_name)\n",
        "    if not os.path.exists(save_model_dir):\n",
        "        os.mkdir(save_model_dir)\n",
        "\n",
        "    for e in trange(int(args.num_train_epochs), desc=\"Epoch\"):\n",
        "        avg_loss = 0.\n",
        "\n",
        "        for step, batch in enumerate(train_dataloader):\n",
        "            batch = tuple(t.cuda() for t in batch) #.cuda()\n",
        "            _, input_ids, input_mask, segment_ids, masked_ids, segment_span, class_span = batch\n",
        "            \"\"\"train generator at each batch\"\"\"\n",
        "            #print(segment_span, class_span)\n",
        "            optimizer.zero_grad() \n",
        "            outputs = model(input_ids, input_mask, segment_ids, labels=masked_ids)\n",
        "            loss = outputs.loss#[0]\n",
        "            loss.backward()\n",
        "            avg_loss += loss.item()\n",
        "            optimizer.step()\n",
        "            if (step + 1) % 50 == 0:\n",
        "                print(\"avg_loss: {}\".format(avg_loss / 50))\n",
        "                avg_loss = 0\n",
        "        if args.save_every_epoch:\n",
        "            save_model_name = \"BertForMaskedLM_\" + task_name + \"_epoch_\" + str(e + 1)\n",
        "            save_model_path = os.path.join(save_model_dir, save_model_name)\n",
        "            torch.save(model, save_model_path)\n",
        "        else:\n",
        "            if (e + 1) % 10 == 0:\n",
        "                save_model_name = \"BertForMaskedLM_\" + task_name + \"_epoch_\" + str(e + 1)\n",
        "                save_model_path = os.path.join(save_model_dir, save_model_name)\n",
        "                torch.save(model, save_model_path)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'dataset': 'subj', 'bilm_mode': 'sampling', 'stop_epoch': 20, 'bilm_wordwise': True, 'bilm_gumbel': False, 'no_label': False, 'dropout': 0.5, 'epoch': 100, 'bilm_dropout': 0.0, 'unit': 300, 'bilm_layer': 1, 'save_model': False, 'learning_rate': 0.0001, 'bilm_residual': 0.0, 'bilm_add_original': 0.0, 'bilm_temp': 1.0, 'out': 'result', 'validation': True, 'seed': 2018, 'bilm_ratio': 0.25, 'layer': 1, 'model': 'rnn', 'bilm': None, 'gpu': 0, 'bilm_unit': 1024, 'resume_vocab': None, 'batchsize': 64}\n",
            "<__main__.Arguments object at 0x7f24660e4590>\n",
            "task_name subj\n",
            "label_list ['0', '1']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-49-8e0031e0c021>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-49-8e0031e0c021>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0;31m## leveraging lastest bert module in Transformers to load pre-trained model tokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m     \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBertTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbert_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"bert\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbert_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   1668\u001b[0m                         \u001b[0mlocal_files_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlocal_files_only\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1669\u001b[0m                         \u001b[0muse_auth_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_auth_token\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1670\u001b[0;31m                         \u001b[0muser_agent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muser_agent\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1671\u001b[0m                     )\n\u001b[1;32m   1672\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/file_utils.py\u001b[0m in \u001b[0;36mcached_path\u001b[0;34m(url_or_filename, cache_dir, force_download, proxies, resume_download, user_agent, extract_compressed_file, force_extract, use_auth_token, local_files_only)\u001b[0m\n\u001b[1;32m   1171\u001b[0m             \u001b[0muser_agent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muser_agent\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1172\u001b[0m             \u001b[0muse_auth_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_auth_token\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1173\u001b[0;31m             \u001b[0mlocal_files_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlocal_files_only\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1174\u001b[0m         )\n\u001b[1;32m   1175\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl_or_filename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/file_utils.py\u001b[0m in \u001b[0;36mget_from_cache\u001b[0;34m(url, cache_dir, force_download, proxies, etag_timeout, resume_download, user_agent, use_auth_token, local_files_only)\u001b[0m\n\u001b[1;32m   1333\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mlocal_files_only\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1334\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1335\u001b[0;31m             \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_redirects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproxies\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mproxies\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0metag_timeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1336\u001b[0m             \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1337\u001b[0m             \u001b[0metag\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"X-Linked-Etag\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ETag\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/requests/api.py\u001b[0m in \u001b[0;36mhead\u001b[0;34m(url, **kwargs)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetdefault\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'allow_redirects'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'head'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/requests/api.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;31m# cases, and look like a memory leak in others.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0msessions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/requests/sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    528\u001b[0m         }\n\u001b[1;32m    529\u001b[0m         \u001b[0msend_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 530\u001b[0;31m         \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    531\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/requests/sessions.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    641\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    642\u001b[0m         \u001b[0;31m# Send the request\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 643\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madapter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    644\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    645\u001b[0m         \u001b[0;31m# Total elapsed time of the request (approximately)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/requests/adapters.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    447\u001b[0m                     \u001b[0mdecode_content\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m                     \u001b[0mretries\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_retries\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 449\u001b[0;31m                     \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    450\u001b[0m                 )\n\u001b[1;32m    451\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    598\u001b[0m                                                   \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout_obj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    599\u001b[0m                                                   \u001b[0mbody\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 600\u001b[0;31m                                                   chunked=chunked)\n\u001b[0m\u001b[1;32m    601\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    602\u001b[0m             \u001b[0;31m# If we're going to release the connection in ``finally:``, then\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    341\u001b[0m         \u001b[0;31m# Trigger any extra validation we need to do.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    342\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 343\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_conn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    344\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mSocketTimeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBaseSSLError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m             \u001b[0;31m# Py2 raises this as a BaseSSLError, Py3 raises it as socket timeout.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_validate_conn\u001b[0;34m(self, conn)\u001b[0m\n\u001b[1;32m    837\u001b[0m         \u001b[0;31m# Force connect early to allow us to validate the connection.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    838\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'sock'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# AppEngine might not have  `.sock`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 839\u001b[0;31m             \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    840\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    841\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_verified\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/urllib3/connection.py\u001b[0m in \u001b[0;36mconnect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    299\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m         \u001b[0;31m# Add certificate verification\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 301\u001b[0;31m         \u001b[0mconn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_new_conn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    302\u001b[0m         \u001b[0mhostname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhost\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/urllib3/connection.py\u001b[0m in \u001b[0;36m_new_conn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m             conn = connection.create_connection(\n\u001b[0;32m--> 159\u001b[0;31m                 (self._dns_host, self.port), self.timeout, **extra_kw)\n\u001b[0m\u001b[1;32m    160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mSocketTimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/urllib3/util/connection.py\u001b[0m in \u001b[0;36mcreate_connection\u001b[0;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0msource_address\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m                 \u001b[0msock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource_address\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0msock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msa\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msock\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "alLbOBPMzPaB"
      },
      "source": [
        "path = '/content/cbert_pretrained/stsa.fine/'\n",
        "file_dir = os.listdir(path)\n",
        "\n",
        "with zipfile.ZipFile('pretrained_cbert.zip', mode='w', compression=zipfile.ZIP_DEFLATED) as zf:\n",
        "    for dirpath, dirnames, filenames in os.walk(path):\n",
        "      for filename in filenames:\n",
        "        print(\":\", os.path.join(dirpath, filename))\n",
        "        add_file = os.path.join(dirpath, filename)\n",
        "        zf.write(add_file)\n",
        "\n",
        "os.system('pretrained_cbert.zip')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "MgNAzD_Wz7nb",
        "outputId": "3d56aa19-a775-43e2-97be-027fddbf9969"
      },
      "source": [
        "shutil.move('/content/pretrained_cbert.zip', '/content/drive/MyDrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/drive/MyDrive/pretrained_cbert.zip'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "ssO_X0mHLdGc",
        "outputId": "b150069b-0b07-414e-e4de-14175315a9ab"
      },
      "source": [
        "zip_file = '/content/drive/MyDrive/pretrained_cbert.zip'\n",
        "\n",
        "z = zipfile.ZipFile(zip_file, 'r')\n",
        "z.extractall()\n",
        "\n",
        "print(os.listdir())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-77fef54efe81>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mzip_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/content/drive/MyDrive/pretrained_cbert.zip'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzipfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZipFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mz\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextractall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/zipfile.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, file, mode, compression, allowZip64, compresslevel)\u001b[0m\n\u001b[1;32m   1238\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1239\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1240\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilemode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1241\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1242\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mfilemode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodeDict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/pretrained_cbert.zip'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Phhpw1-BeO7Q",
        "outputId": "1d4fbb0b-4207-4cdd-b584-f249ed5ac12c"
      },
      "source": [
        "#test\n",
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import csv\n",
        "import os\n",
        "import shutil\n",
        "import logging\n",
        "import argparse\n",
        "import random\n",
        "from tqdm import tqdm, trange\n",
        "import json\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler\n",
        "\n",
        "from transformers import BertTokenizer, BertModel, BertForMaskedLM, AdamW\n",
        "\n",
        "#import cbert_utils\n",
        "import train_text_classifier\n",
        "\n",
        "#PYTORCH_PRETRAINED_BERT_CACHE = \".pytorch_pretrained_bert\"\n",
        "\n",
        "\"\"\"initialize logger\"\"\"\n",
        "logging.basicConfig(format='%(asctime)s - %(levelname)s - %(name)s -   %(message)s',\n",
        "                    datefmt='%m/%d/%Y %H:%M:%S',\n",
        "                    level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "\"\"\"cuda or cpu\"\"\"\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "def convert_ids_to_str(ids, tokenizer):\n",
        "    \"\"\"converts token_ids into str.\"\"\"\n",
        "    tokens = []\n",
        "    for token_id in ids:\n",
        "        token = tokenizer._convert_id_to_token(token_id)\n",
        "        tokens.append(token)\n",
        "    outputs = rev_wordpiece(tokens)\n",
        "    return outputs\n",
        "\n",
        "def main():\n",
        "    #parser = argparse.ArgumentParser()\n",
        "\n",
        "    ## required parameters\n",
        "    #parser.add_argument(\"--data_dir\", default=\"datasets\", type=str,\n",
        "    #                    help=\"The input data dir. Should contain the .tsv files (or other data files) for the task.\")\n",
        "    #parser.add_argument(\"--output_dir\", default=\"aug_data\", type=str,\n",
        "    #                    help=\"The output dir for augmented dataset\")\n",
        "    #parser.add_argument(\"--save_model_dir\", default=\"cbert_model\", type=str,\n",
        "    #                    help=\"The cache dir for saved model.\")\n",
        "    #parser.add_argument(\"--bert_model\", default=\"bert-base-uncased\", type=str,\n",
        "    #                    help=\"The path of pretrained bert model.\")\n",
        "    #parser.add_argument(\"--task_name\", default=\"subj\",type=str,\n",
        "    #                    help=\"The name of the task to train.\")\n",
        "    #parser.add_argument(\"--max_seq_length\", default=64, type=int,\n",
        "    #                    help=\"The maximum total input sequence length after WordPiece tokenization. \\n\"\n",
        "    #                         \"Sequences longer than this will be truncated, and sequences shorter \\n\"\n",
        "    #                         \"than this will be padded.\")\n",
        "    #parser.add_argument(\"--do_lower_case\", default=False, action='store_true',\n",
        "    #                    help=\"Set this flag if you are using an uncased model.\")\n",
        "    #parser.add_argument(\"--train_batch_size\", default=32, type=int,\n",
        "    #                    help=\"Total batch size for training.\")\n",
        "    #parser.add_argument(\"--learning_rate\", default=5e-5, type=float,\n",
        "    #                    help=\"The initial learning rate for Adam.\")\n",
        "    #parser.add_argument(\"--num_train_epochs\", default=9.0, type=float,\n",
        "    #                    help=\"Total number of training epochs to perform.\")\n",
        "    #arser.add_argument(\"--warmup_proportion\", default=0.1, type=float,\n",
        "    #                    help=\"Proportion of training to perform linear learning rate warmup for. \"\n",
        "    #                         \"E.g., 0.1 = 10%% of training.\")\n",
        "    #parser.add_argument('--seed', default=42, type=int, \n",
        "    #                    help=\"random seed for initialization\")\n",
        "    #parser.add_argument('--sample_num', default=1, type=int,\n",
        "    #                    help=\"sample number\")\n",
        "    #parser.add_argument('--sample_ratio', default=7, type=int,\n",
        "    #                    help=\"sample ratio\")\n",
        "    #parser.add_argument('--gpu', default=0, type=int,\n",
        "    #                    help=\"gpu id\")\n",
        "    #parser.add_argument('--temp', default=1.0, type=float,\n",
        "    #                    help=\"temperature\")\n",
        "    \n",
        "\n",
        "    #args = parser.parse_args()\n",
        "\n",
        "    args = Arguments()\n",
        "    args.save_model_dir = \"/content/drive/MyDrive/pretrained_cbert\"\n",
        "    \n",
        "    \"\"\"prepare processors\"\"\"\n",
        "    #augProcessor = AugProcessor()\n",
        "\n",
        "    with open(\"global.config\", 'r') as f:\n",
        "        configs_dict = json.load(f)\n",
        "\n",
        "    args = Arguments()\n",
        "    args.task_name = configs_dict.get(\"dataset\")\n",
        "    args.output_dir = args.output_dir + '_{}_{}_{}_{}'.format(args.sample_num, args.sample_ratio, args.gpu, args.temp)\n",
        "    print(args)\n",
        "    \n",
        "    \"\"\"prepare processors\"\"\"\n",
        "    augProcessor = AugProcessor() #cbert_utils.\n",
        "    processors = {\n",
        "        ## you can add your processor here\n",
        "        \"TREC\": augProcessor,\n",
        "        \"stsa.fine\": augProcessor,\n",
        "        \"stsa.binary\": augProcessor,\n",
        "        \"mpqa\": augProcessor,\n",
        "        \"rt-polarity\": augProcessor,\n",
        "        \"subj\": augProcessor,\n",
        "        \"amazon\": augProcessor,\n",
        "    }\n",
        "\n",
        "    task_name = args.task_name\n",
        "    if task_name not in processors:\n",
        "        raise ValueError(\"Task not found: %s\" % (task_name))\n",
        "    processor = processors[task_name]\n",
        "    label_list = processor.get_labels(task_name)\n",
        "\n",
        "    ## prepare for model\n",
        "    random.seed(args.seed)\n",
        "    np.random.seed(args.seed)\n",
        "    torch.manual_seed(args.seed)\n",
        "    tokenizer = BertTokenizer.from_pretrained(args.bert_model, do_lower_case=args.do_lower_case)\n",
        "\n",
        "    def load_model(model_name):\n",
        "        weights_path = os.path.join(args.save_model_dir, model_name)\n",
        "        print(\"WP\", weights_path)\n",
        "        model = torch.load(weights_path)\n",
        "        return model\n",
        "    \n",
        "    args.data_dir = os.path.join(args.data_dir, task_name)\n",
        "    print(\"Input dir \", args.data_dir)\n",
        "    args.output_dir = os.path.join(args.output_dir, task_name)\n",
        "    print(\"Output dir \", args.output_dir)\n",
        "    if os.path.exists(args.output_dir):\n",
        "        shutil.rmtree(args.output_dir)\n",
        "    shutil.copytree(\"aug_data/{}\".format(task_name), args.output_dir)\n",
        "\n",
        "    ## prepare for training\n",
        "    train_examples = processor.get_train_examples(args.data_dir)\n",
        "    train_features, num_train_steps, train_dataloader = \\\n",
        "        construct_train_dataloader(train_examples, label_list, args.max_seq_length, \n",
        "        args.train_batch_size, args.num_train_epochs, tokenizer, device)\n",
        "\n",
        "    logger.info(\"***** Running training *****\")\n",
        "    logger.info(\"  Num examples = %d\", len(train_examples))\n",
        "    logger.info(\"  Batch size = %d\", args.train_batch_size)\n",
        "    logger.info(\"  Num steps = %d\", num_train_steps)\n",
        "\n",
        "    save_model_dir = os.path.join(args.save_model_dir, task_name)\n",
        "    if not os.path.exists(save_model_dir):\n",
        "        os.mkdir(save_model_dir)\n",
        "    MASK_id = convert_tokens_to_ids(['[MASK]'], tokenizer)[0]\n",
        "\n",
        "    #origin_train_path = os.path.join(args.output_dir, \"train_origin.tsv\")\n",
        "    save_train_path = os.path.join(args.output_dir, \"train.tsv\")\n",
        "    #shutil.copy(origin_train_path, save_train_path)\n",
        "    #best_test_acc = train_text_classifier.train(\"aug_data_{}_{}_{}_{}\".format(args.sample_num, args.sample_ratio, args.gpu, args.temp))\n",
        "    #print(\"before augment best acc:{}\".format(best_test_acc))\n",
        "\n",
        "    output_examples = []\n",
        "\n",
        "    for e in trange(int(args.num_train_epochs), desc=\"Epoch\"):\n",
        "        torch.cuda.empty_cache()\n",
        "        cbert_name = \"{}/BertForMaskedLM_{}_epoch_{}\".format(task_name.lower(), task_name.lower(), e+1)\n",
        "        model = load_model(cbert_name)\n",
        "        model.cuda()\n",
        "        #shutil.copy(origin_train_path, save_train_path)\n",
        "        #save_train_path = ''\n",
        "        save_train_file = open(save_train_path, 'a')\n",
        "        tsv_writer = csv.writer(save_train_file, delimiter='\\t')\n",
        "        for _, batch in enumerate(train_dataloader):\n",
        "            model.eval()\n",
        "            batch = tuple(t.cuda() for t in batch)\n",
        "            init_ids, _, input_mask, segment_ids, _, segmnet_span, class_span = batch\n",
        "            #input_lens = [sum(mask).item() for mask in input_mask]\n",
        "            #masked_idx = np.squeeze([np.random.randint(0, l, max(l//args.sample_ratio, 1)) for l in input_lens])\n",
        "            #for ids, idx in zip(init_ids, masked_idx):\n",
        "            #    ids[idx] = MASK_id\n",
        "            with torch.no_grad():\n",
        "              predictions = model(init_ids, input_mask, segment_ids)\n",
        "              hidden_states = predictions.hidden_states[2]\n",
        "              #print(\"hidden_states\", hidden_states.shape)\n",
        "              sentence_embedding = torch.mean(hidden_states[-1], dim=0).squeeze()\n",
        "              #print(sentence_embedding)\n",
        "              #print(sentence_embedding.size())\n",
        "              output_examples.append(OutputObject(embedding = sentence_embedding.detach().cpu().numpy(), segment = segmnet_span.detach().cpu().numpy(), cl = class_span.detach().cpu().numpy()))\n",
        "              #print(output_examples)\n",
        "              #np.savetxt('tensor.txt', sentence_embedding.detach().cpu().numpy())\n",
        "              \n",
        "          #    # get last four layers\n",
        "          #    last_four_layers = [hidden_states[i] for i in (-1, -2, -3, -4)]\n",
        "          #    # cast layers to a tuple and concatenate over the last dimension\n",
        "          #    cat_hidden_states = torch.cat(tuple(last_four_layers), dim=-1)\n",
        "          #    print(cat_hidden_states.size())\n",
        "\n",
        "              # take the mean of the concatenated vector over the token dimension\n",
        "          #    cat_sentence_embedding = torch.mean(cat_hidden_states, dim=1).squeeze()\n",
        "          #    print(cat_sentence_embedding)\n",
        "          #    print(cat_sentence_embedding.size())  \n",
        "            \n",
        "            for out_ex in output_examples:\n",
        "                #preds = torch.multinomial(preds, args.sample_num, replacement=True)[idx]\n",
        "                #if len(preds.size()) == 2:\n",
        "                #    preds = torch.transpose(preds, 0, 1)\n",
        "                #for pred in preds:\n",
        "                #    ids[idx] = pred\n",
        "                #    new_str = convert_ids_to_str(ids.cpu().numpy(), tokenizer)\n",
        "                tsv_writer.writerow(np.hstack([out_ex.embedding, int(out_ex.segment[0]), int(out_ex.cl[0])]))\n",
        "            torch.cuda.empty_cache()\n",
        "        \n",
        "        #predictions = predictions.detach().cpu()\n",
        "        #model.cpu()\n",
        "        #torch.cuda.empty_cache()\n",
        "        #bak_train_path = os.path.join(args.output_dir, \"train_epoch_{}.tsv\".format(e))\n",
        "        #shutil.copy(save_train_path, bak_train_path)\n",
        "        #best_test_acc = train_text_classifier.train(\"aug_data_{}_{}_{}_{}\".format(args.sample_num, args.sample_ratio, args.gpu, args.temp))\n",
        "        #print(\"epoch {} augment best acc:{}\".format(e, best_test_acc))\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<__main__.Arguments object at 0x7f24660edc90>\n",
            "Amazon\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "04/20/2021 11:25:55 - INFO - __main__ - [cbert] *** Example ***\n",
            "04/20/2021 11:25:55 - INFO - __main__ - [cbert] guid: train-1\n",
            "04/20/2021 11:25:55 - INFO - __main__ - [cbert] tokens: [CLS] school uniforms is good . [UNK] drag a student over the fence from outside the school grounds . [SEP]\n",
            "04/20/2021 11:25:55 - INFO - __main__ - [cbert] init_ids: 101 2082 11408 2003 2204 1012 100 8011 1037 3076 2058 1996 8638 2013 2648 1996 2082 5286 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "04/20/2021 11:25:55 - INFO - __main__ - [cbert] input_ids: 101 2082 11408 2003 2204 1012 100 8011 103 3076 2058 1996 8638 2013 2648 1996 2082 5286 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "04/20/2021 11:25:55 - INFO - __main__ - [cbert] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "04/20/2021 11:25:55 - INFO - __main__ - [cbert] segment_ids: 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "04/20/2021 11:25:55 - INFO - __main__ - [cbert] masked_lm_labels: -100 -100 -100 -100 -100 -100 -100 -100 1037 -100 -100 1996 8638 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
            "04/20/2021 11:25:55 - INFO - __main__ - [cbert] *** Example ***\n",
            "04/20/2021 11:25:55 - INFO - __main__ - [cbert] guid: train-2\n",
            "04/20/2021 11:25:55 - INFO - __main__ - [cbert] tokens: [CLS] school uniforms is good . [SEP]\n",
            "04/20/2021 11:25:55 - INFO - __main__ - [cbert] init_ids: 101 2082 11408 2003 2204 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "04/20/2021 11:25:55 - INFO - __main__ - [cbert] input_ids: 101 2082 11408 2003 2204 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "04/20/2021 11:25:55 - INFO - __main__ - [cbert] input_mask: 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "04/20/2021 11:25:55 - INFO - __main__ - [cbert] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "04/20/2021 11:25:55 - INFO - __main__ - [cbert] masked_lm_labels: -100 -100 -100 -100 -100 1012 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
            "04/20/2021 11:25:55 - INFO - __main__ - [cbert] *** Example ***\n",
            "04/20/2021 11:25:55 - INFO - __main__ - [cbert] guid: train-3\n",
            "04/20/2021 11:25:55 - INFO - __main__ - [cbert] tokens: [CLS] school uniforms is good . [SEP]\n",
            "04/20/2021 11:25:55 - INFO - __main__ - [cbert] init_ids: 101 2082 11408 2003 2204 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "04/20/2021 11:25:55 - INFO - __main__ - [cbert] input_ids: 101 2082 11408 2003 2204 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "04/20/2021 11:25:55 - INFO - __main__ - [cbert] input_mask: 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "04/20/2021 11:25:55 - INFO - __main__ - [cbert] segment_ids: 2 2 2 2 2 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "04/20/2021 11:25:55 - INFO - __main__ - [cbert] masked_lm_labels: -100 -100 -100 -100 -100 1012 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
            "04/20/2021 11:25:55 - INFO - __main__ - [cbert] *** Example ***\n",
            "04/20/2021 11:25:55 - INFO - __main__ - [cbert] guid: train-4\n",
            "04/20/2021 11:25:55 - INFO - __main__ - [cbert] tokens: [CLS] [UNK] [SEP]\n",
            "04/20/2021 11:25:55 - INFO - __main__ - [cbert] init_ids: 101 100 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "04/20/2021 11:25:55 - INFO - __main__ - [cbert] input_ids: 101 103 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "04/20/2021 11:25:55 - INFO - __main__ - [cbert] input_mask: 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "04/20/2021 11:25:55 - INFO - __main__ - [cbert] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "04/20/2021 11:25:55 - INFO - __main__ - [cbert] masked_lm_labels: -100 100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
            "04/20/2021 11:25:55 - INFO - __main__ - [cbert] *** Example ***\n",
            "04/20/2021 11:25:55 - INFO - __main__ - [cbert] guid: train-5\n",
            "04/20/2021 11:25:55 - INFO - __main__ - [cbert] tokens: [CLS] for prostitutes is the school uniform an obligatory part of their professional wardrobe ( and one may wonder [SEP]\n",
            "04/20/2021 11:25:55 - INFO - __main__ - [cbert] init_ids: 101 2005 20833 2003 1996 2082 6375 2019 26471 2112 1997 2037 2658 17828 1006 1998 2028 2089 4687 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "04/20/2021 11:25:55 - INFO - __main__ - [cbert] input_ids: 101 2005 20833 2003 1996 2082 6375 2019 103 2112 1997 2037 2658 17828 1006 1998 2028 2089 4687 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "04/20/2021 11:25:55 - INFO - __main__ - [cbert] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "04/20/2021 11:25:55 - INFO - __main__ - [cbert] segment_ids: 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "04/20/2021 11:25:55 - INFO - __main__ - [cbert] masked_lm_labels: -100 -100 -100 -100 -100 -100 -100 -100 26471 -100 -100 2037 2658 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
            "04/20/2021 11:25:56 - INFO - __main__ - ***** Running training *****\n",
            "04/20/2021 11:25:56 - INFO - __main__ -   Num examples = 35\n",
            "04/20/2021 11:25:56 - INFO - __main__ -   Batch size = 32\n",
            "04/20/2021 11:25:56 - INFO - __main__ -   Num steps = 10\n",
            "Epoch:   0%|          | 0/10 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Input dir  datasets/amazon\n",
            "Output dir  aug_data_1_7_0_1.0/amazon\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  10%|         | 1/10 [00:00<00:07,  1.25it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WP cbert_pretrained/amazon/BertForMaskedLM_amazon_epoch_1\n",
            "WP cbert_pretrained/amazon/BertForMaskedLM_amazon_epoch_2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  20%|        | 2/10 [00:01<00:06,  1.33it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WP cbert_pretrained/amazon/BertForMaskedLM_amazon_epoch_3\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  30%|       | 3/10 [00:02<00:05,  1.39it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WP cbert_pretrained/amazon/BertForMaskedLM_amazon_epoch_4\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  40%|      | 4/10 [00:02<00:04,  1.43it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WP cbert_pretrained/amazon/BertForMaskedLM_amazon_epoch_5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  50%|     | 5/10 [00:03<00:03,  1.45it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WP cbert_pretrained/amazon/BertForMaskedLM_amazon_epoch_6\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  60%|    | 6/10 [00:04<00:02,  1.48it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WP cbert_pretrained/amazon/BertForMaskedLM_amazon_epoch_7\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  70%|   | 7/10 [00:04<00:02,  1.50it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WP cbert_pretrained/amazon/BertForMaskedLM_amazon_epoch_8\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  80%|  | 8/10 [00:05<00:01,  1.51it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WP cbert_pretrained/amazon/BertForMaskedLM_amazon_epoch_9\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  90%| | 9/10 [00:06<00:00,  1.50it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WP cbert_pretrained/amazon/BertForMaskedLM_amazon_epoch_10\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 100%|| 10/10 [00:06<00:00,  1.50it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 165
        },
        "id": "vnOInfoizw_o",
        "outputId": "2f8d8528-1def-4ec9-cb1a-4dd52a70e301"
      },
      "source": [
        "output_examples"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-53-506191ba6055>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0moutput_examples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'output_examples' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F5ZGtYq_a4qI"
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UYst0nK34jxf"
      },
      "source": [
        "from transformers import BertTokenizer, BertForMaskedLM\n",
        "import torch\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertForMaskedLM.from_pretrained('bert-base-uncased')\n",
        "\n",
        "inputs = tokenizer(\"The capital of France is [MASK].\", return_tensors=\"pt\")\n",
        "labels = tokenizer(\"The capital of France is Paris.\", return_tensors=\"pt\")[\"input_ids\"]\n",
        "\n",
        "outputs = model(**inputs, labels=labels)\n",
        "loss = outputs.loss\n",
        "logits = outputs.logits"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vAQMrAobwWUf"
      },
      "source": [
        "loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q-XDVcn-wdJQ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}